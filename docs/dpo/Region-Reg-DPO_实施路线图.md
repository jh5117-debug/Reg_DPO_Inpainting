# Region-Reg-DPO 实施路线图

> 基于 `/home/hj/Diffueraser_test` 代码库的逐步实施指南
> 每个步骤均标注可能的坑和注意事项，不含任何证明

---

## Step 0：准备训练数据集（P0，3-4 周，最重要）

**做什么**：用现有推理流水线批量生成偏好对数据

1. 用 `create_random_shape_with_random_motion` 对训练视频生成随机 mask 序列
2. 正样本 = 原始未遮挡帧（GT）
3. 负样本三类分批生成：
   - **幻觉负样本**：跑 DiffuEraser 推理但 **关掉 Prior 注入**（`priori=None`）
   - **模糊负样本**：只跑 ProPainter，不跑 DiffuEraser
   - **闪烁负样本**：正常跑 DiffuEraser 长视频，截取 clip 边界帧

**⚠️ 坑与注意事项**：

| # | 问题 | 说明 |
|---|------|------|
| 1 | Mask 分布不匹配 | 随机 mask 必须和推理时的 mask 分布接近（大小、形状、运动模式），否则训练/推理分布错位 |
| 2 | 负样本太好 | 正负样本差距必须 **肉眼可见**，如果负样本质量太高（接近 GT），DPO 学不到东西 |
| 3 | 数据量不足 | 至少 **1000+ 视频 clip**（每 clip 22帧），太少会过拟合 |
| 4 | 这一步是瓶颈 | 负样本质量直接决定最终效果，**不要急着写训练代码** |

---

## Step 1：实现三区 Mask 分解（0.5 周）

**做什么**：写一个工具函数，输入原始 mask，输出 $M_h, M_b, M_c$ 三个 mask

```python
M_b = Dilate(M_h, k) - M_h   # k=5~15px
M_c = 1 - M_h - M_b
```

**⚠️ 坑与注意事项**：

| # | 问题 | 说明 |
|---|------|------|
| 1 | 和 Blending 膨胀混淆 | `diffueraser_OR.py` 里已有 `mask_dilation_iter` 做膨胀——那是 Blending 用的，你这个是 loss 用的，**两套独立** |
| 2 | Latent space 分辨率 | Loss 在 latent space 计算，分辨率是原图的 **1/8**，膨胀核要相应缩小 |
| 3 | 互斥性 | 三个 mask 必须严格互斥且 $M_h + M_b + M_c = 1$，不能有重叠 |

---

## Step 2：实现 Region-Reg-DPO Loss（1-2 周）

**做什么**：新建 `region_reg_dpo_loss.py`，实现损失函数

核心就两项相加：
- **Region-DPO 项**：$-\log\sigma(-\beta' \cdot s_{\text{region}})$
- **分区 SFT 项**：$\sum_r \rho_r \|M_r \odot (\epsilon^w - \epsilon_\theta^w)\|^2$

**⚠️ 坑与注意事项**：

| # | 问题 | 说明 |
|---|------|------|
| 1 | 参考模型梯度泄露 | $\epsilon_{\text{ref}}$ 来自冻结的参考模型，前向传播时必须 `with torch.no_grad()` |
| 2 | 漏乘时间步权重 | $\beta' = \beta \cdot T \cdot \omega(\lambda_t)$，不同 timestep 的梯度量级差几个数量级 |
| 3 | 初始 DGR 脉冲 | 初始几步 DGR 极大（~500），容易一步把 $s_{\text{region}}$ 打负导致后续 DGR 归零。用 **gradient clipping** 保护 |
| 4 | 数值稳定性 | $\log\sigma(x)$ 在 $x$ 极大/极小时要用 `F.logsigmoid` 避免溢出 |

---

## Step 3：搭建 Stage 3 训练脚本（1 周）

**做什么**：基于现有 `train_DiffuEraser_stage1.py` 改写 `train_DiffuEraser_stage3_dpo.py`

**关键改动**：
1. 给主 UNet 加 **LoRA**（rank=16, α=32），只训练 LoRA 参数
2. **冻结**：BrushNet 全冻结、运动模块全冻结
3. 加载 Stage 2 checkpoint 作为参考模型（冻结）
4. 每个 batch 需要同时前向传播：**当前模型 × 正负样本 + 参考模型 × 正负样本 = 4 次前向**
5. 损失函数换成 Step 2 的 Region-Reg-DPO Loss

**⚠️ 坑与注意事项**：

| # | 问题 | 说明 |
|---|------|------|
| 1 | **显存爆炸** | 4 次前向 + 22 帧是显存杀手，必须同时做：LoRA / 参考模型 CPU offload / gradient checkpointing / bf16 mixed precision |
| 2 | Prior 注入不能省 | DPO 训练时正负样本都要经过 DDIM Inversion 得到初始噪声，**不能用随机噪声替代** |
| 3 | DataLoader 复杂度倍增 | 要同时返回：原始帧、模型补全帧、mask、prior，格式比 Stage 1 复杂得多 |
| 4 | LoRA 位置 | 只加在 UNet 的 attention 层（QKV + out proj），不要加在 conv 层 |

---

## Step 4：小规模验证（1 周）

**做什么**：用 50-100 个 clip 跑几百步，验证训练是否稳定

**必须监控的 4 个指标**：

| 指标 | 正常范围 | 异常处理 |
|------|---------|---------|
| `DGR` 值 | 从 ~500 平滑下降到个位数 | 前 10 步就归零 → $\beta$ 太大或 lr 太大 |
| `s_region` 值 | 缓慢变负（$-0.001$ ~ $-0.01$） | 剧烈跳变 → gradient clipping 没生效 |
| 各区域 SFT loss | 稳定下降 | 不降 → 检查参考模型是否真冻结了 |
| 生成样本质量 | 每 100 步存推理结果对比 | 色偏/崩溃 → LoRA rank 太大或 $\rho_r$ 太小 |

---

## Step 5：分阶段调参（2-3 周）

**严格按以下顺序调，不可跳步**：

| 轮次 | 配置 | 观察目标 |
|------|------|----------|
| 第1轮 | 所有 $\rho_r = 0$，只有 Region-DPO | DGR 是否稳定？纯 DPO 能否跑通？ |
| 第2轮 | 加统一 $\rho = 100$ | 训练稳定性是否改善？ |
| 第3轮 | 分区 $\rho_b=200, \rho_h=100, \rho_c=0$ | 边界质量是否提升？接缝是否消除？ |
| 第4轮 | 微调 $\alpha_b$ 和 $\rho_b$ 比例 | 寻找最优平衡点 |

**⚠️ 坑与注意事项**：

| # | 问题 | 说明 |
|---|------|------|
| 1 | 一步到位 | **不要一开始就上全部参数**，先确认纯 DPO（$\rho=0$）能跑通 |
| 2 | $\rho_b$ 过大 | 会让边界"过于保守"——补全内容和原始像素完全一样，等于没补 |
| 3 | $\alpha_c$ 不能为 0 | 设成恰好 0 会导致上下文区域完全不参与 DPO，可设 0.05~0.1 |

---

## Step 6：全量训练 + 评估（3-4 周）

**做什么**：用最优超参跑完整训练，然后在测试集上评估

**评估指标**（⚠️ 只在 mask 区域内算，不是全帧）：

| 维度 | 指标 | 说明 |
|------|------|------|
| 补全质量 | PSNR / SSIM / LPIPS | mask 区域内计算 |
| 时序一致性 | 光流一致性误差 | 相邻帧 mask 区域像素变化 |
| 幻觉抑制 | 幻觉检测率 | VLM 检测异常物体 |
| 人类评估 | A/B 对比 | 优化前 vs 优化后 |

**⚠️ 坑与注意事项**：

| # | 问题 | 说明 |
|---|------|------|
| 1 | 忘加 LoRA | 推理时 `diffueraser_OR.py` 里要加 LoRA merge 逻辑，否则用的还是原模型 |
| 2 | 指标计算区域错误 | 全帧 PSNR 会被上下文区域的高分拉高，掩盖 mask 区域问题 |
| 3 | 和 ProPainter 对比 | 别只和原始 DiffuEraser 比，也要和纯 ProPainter 对比，确认没有回退 |

---

## 🔑 总结：最关键的 3 件事

| 优先级 | 关键点 | 一句话 |
|--------|--------|--------|
| **#1** | 数据先行 | Step 0 的负样本质量决定一切，宁可多花 2 周做数据 |
| **#2** | 显存管理 | 4 次前向 × 22 帧是显存杀手，LoRA + offload + checkpointing 缺一不可 |
| **#3** | DGR 监控 | 训练时实时打印 DGR 值，它归零了就等于白训 |

---

## 📋 快速参数速查表

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| $\alpha_h$ | 1.0 | 洞内偏好基准 |
| $\alpha_b$ | 1.5~2.0 | 边界偏好优先 |
| $\alpha_c$ | 0.05~0.1 | 上下文几乎不参与 |
| $\rho_h$ | 50~150 | 洞内中等锚定 |
| $\rho_b$ | 100~250 | 边界最强锚定 |
| $\rho_c$ | 0~10 | 上下文由 Blending 保护 |
| $\beta$ | 500~1000 | 先用 500 |
| lr | $1\times10^{-6}$ ~ $5\times10^{-6}$ | 偏小更安全 |
| LoRA rank | 16 | α=32 |
| 膨胀核 $k$ | 5~15 px | 边界带宽度 |
| clip 长度 | 22 帧 | 与推理一致 |
