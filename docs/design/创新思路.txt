
Text prompt 对 video inpainting 效果不好是可以理解的。Inpainting 需要的是局部细粒度的视觉信息（纹理、光照、空间结构真实像素），而文本描述天然是高层语义的、粗粒度的，很难精确描述（miáo shù）"被 mask 掉的区域应该长什么样"。用图像表示来提供视觉条件，信息量确实比文本丰富得多。
单帧表示不够用。 视频 inpainting 需要的是时空一致的补全。假设你选第 1 帧的表示作为条件，那如果被 mask 的物体在后面的帧里遮住了不同的背景区域呢？单帧的全局表示无法覆盖所有帧的局部缺失信息。
对于视频来说其实 prompt的单帧全局都是表示全局局部信息的这种情况是一种现在需要解决的点,目前来说有没有什么办法,可能就是类似于RCG这种方式,多维度的注入模型 且这些注入的东西都带有时序能力
方向一：逐帧提取空间特征图 + 时序聚合
不再用一个全局向量代表整段视频，而是对每一帧分别提取保留空间分辨率的特征图，然后在时间维度上做聚合。
核心思路是：每一帧的 mask 区域信息缺失，但不同帧的缺失区域不同（尤其是 OR 中物体在运动）。如果把多帧的特征图在时间维度上对齐、聚合，那么某一帧 mask 区域缺失的信息，可以从其他帧的同一空间位置的特征中补回来。

```
帧1: [可见][可见][缺失][可见]    →  特征图1
帧2: [可见][缺失][可见][可见]    →  特征图2  
帧3: [缺失][可见][可见][可见]    →  特征图3

时序聚合后: 每个空间位置都能从至少一帧获得有效特征
```

具体做法可以是：用光流把多帧的特征 warp 到当前帧的坐标系下，然后用一个 temporal attention 或可见性加权平均来融合。可见像素的特征权重高，被 mask 遮挡的像素特征权重低。这本质上是在特征空间做了 ProPainter 在像素空间做的事情，但特征空间的传播比像素更鲁棒，因为高层特征对小的几何误差更宽容。
这种方向应该怎么去优化我的代码呢
第一层（零成本）：复用 ProPainter 的光流。 ProPainter 内部已经用 RAFT 计算了双向光流并做了补全，只需要在 forward() 里多返回 pred_flows_bi，完全不增加计算量。
第二层（特征聚合）： FeatureAggregator 用光流追踪每个 mask 像素在其他帧的对应位置，判断是否可见。如果可见就 warp 过来做加权聚合，生成比 ProPainter 更好的 priori。同时输出一张 visibility_map 告诉下游每个像素的信息完整度。
第三层（自适应扩散）： 在 DiffuEraser 加噪时，用 visibility_map 缩放噪声——可见性高的像素几乎不加噪（保护 priori），可见性低的像素正常加噪（让扩散模型生成）。这直接解决了"DiffuEraser 在高可见性区域帮倒忙"的问题。
integration_guide.md 里有逐行的修改说明和参数调优建议。建议先在几个视频上测试，重点观察 visibility_map 的分布是否符合你之前的经验判断（相机静止→低可见性，物体快速运动→高可见性）。